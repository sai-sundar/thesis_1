% Chapter 1

\chapter{Conclusions and Perspectives} % Main chapter title

\label{Chapter6} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------
The focus in this thesis has been to utilize established neural networks with proposed modifications to suit various aspects of tomographic image reconstruction. A three-stage framework was presented in the form of DUG-RECON, in which each of the stages has a task specific neural network. The first network is a U-Net with a residual connection that denoises the sinogram, the second network, U-Net without the residual connection, maps the denoised sinogram to a reconstruction image estimate and the final stage improves the quality of the image estimate with a residual block. All the neural networks involved are based on \acp{CNN} without any \ac{FC} layers, making it relatively easy for training. The results were quantitatively analyzed and compared with traditional reconstruction approaches and also a deep learning based direct image reconstruction method DeepPET. The second proposed method \ac{LRRCED} demonstrated with DenseNet and U-Net for sparse-view CT reconstruction, uses information from sinogram and low-resolution \ac{FBP} estimates to produce a reconstructed image. This method was also validated on real clinical data for sparse-view CT problem. An ablation study was performed to highlight the impact of different components of the LRRCED. Additionally, we tried to address instability in neural networks with different sinogram sampling as pointed out by \cite{antun2020instabilities}. DenseNet, U-Net and ResNet were the base neural network architectures utilized in this thesis. The proposed changes suggested in both the aforementioned approaches, made them specifically suitable for tomographic image reconstruction. 

One of the key challenges involved with training neural networks is fine-tuning the hyper-parameters. Some of the hyper-parameters like number of filters in the first layer, the factor of multiplicity of filters can be inspired from established benchmarks in tasks like segmentation. Once the hyper-parameters related to the design are fixed, questions related to data and the training duration can be addressed. Though there is a clear interdependence of these hyper-parameters, network design is a more straightforward problem to address thanks to the already existing literature. Typically the strategy used to select the hyper-parameters is to monitor the loss on the validation dataset. Sometimes additional metrics different to that of the loss function are also used to help in fine-tuning the hyper-parameters (\cite{zhang2018sparse}). In image reconstruction as the task is to estimate an image, visual inspection also provides an added advantage to monitor the network training at intermediate steps. 

Data preparation is another important aspect which needs to be addressed before the network can actually be trained. Typically the publicly available human patient data is in the DICOM format which needs to pre-processed and then stored efficiently in formats suitable for the machine learning library. The machine learning library used in this thesis was TensorFlow. Since in image reconstruction the final values in the image pixel are important, normalization of data needs to be carefully managed. It is a common practice to normalize the data prior to training a \ac{CNN} as it helps in faster convergence. However, the scaling required to get back to the original values can lead to loss of information. The range of values in the images are very different for \ac{PET} and \ac{CT}. In \ac{PET} we estimate the tracer activity distribution while in \ac{CT} we estimate the attenuation. The former has large values in the orders of $10^4-10^6$ subject to the dose and the tracer, while the latter has lower values in the order of $10^{-2}$ depending on the energy of the X-rays. For un-normalized data the last layer of the \ac{CNN} needs to have either a linear activation function or a version of ReLU for the estimate predicted by the network to have values in the same range as the image. It is to be noted that convergence could be effected for un-normalized data and the network may need to be trained for a higher number of epochs. For \ac{CT} imaging, a format often used for displaying is the HUT, different HUT windows are used to observe organ specific details. However, a network is typically trained with attenuation images, and a conversion is required to display the images. Hence, the range of values in the images estimated become very important.

Transfer learning in neural network terminology refers to taking a network trained on a large dataset and fine-tuning it's weights to smaller datasets. It is often used in cases where there is dearth of data. It also becomes an important strategy to make the network adapt to changes in the data environment (for example different acquisition geometry). All the weights if the network could be updated by training on the smaller dataset or only the weights of the last few layers. We chose to update the weights of all the layers across the network. The LRRCED was initialized with the weights of the larger semi-simulated dataset, (real patient images-synthesized sinograms) and then trained on the Mayo CT clinical dataset. The reconstructed images reflected the quantitative metrics similar to that of the simulated dataset. With one of the primary challenges of neural network approaches being generalization to new data, transfer learning seems to be a viable option for supervised learning. 

The most commonly used datasets for \ac{PET} and \ac{CT} image reconstruction problems are BrainWeb (\cite{cocosco1997brainweb}) and Mayo Clinic database (\cite{moen2021low}) respectively. However, there is a lack of a bench-marking dataset designed specifically to test deep learning-based approaches. The various data-driven methods proposed across the years use different datasets and data preparation techniques making it difficult to reproduce the results for fair comparison. Even the hyper-parameter fine-tuning gets challenging when the source code and the dataset are not made public. Standardization of data and comparison criteria is paramount to establish state of the art methods through fair and universally accepted evaluation.




